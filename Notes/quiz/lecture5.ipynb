{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcq = [{\"question\": \"What is the primary function of a Transformer model?\", \"opt1\": \"Image classification\", \"opt2\": \"Sequence-to-sequence modeling\", \"opt3\": \"Time series forecasting\", \"opt4\": \"Sentiment analysis\"}, {\"question\": \"What is a key characteristic of Transformer models regarding output length?\", \"opt1\": \"Fixed and predetermined\", \"opt2\": \"Variable and determined by the model\", \"opt3\": \"Dependent on input length\", \"opt4\": \"Independent of input sequence\"}, {\"question\": \"What technique do Transformer models primarily use instead of recurrence for sequential processing?\", \"opt1\": \"Convolution\", \"opt2\": \"Self-attention\", \"opt3\": \"Pooling\", \"opt4\": \"Recurrent connections\"}, {\"question\": \"What is the main purpose of the encoder in a Transformer model?\", \"opt1\": \"To generate the final output\", \"opt2\": \"To process and transform the input sequence\", \"opt3\": \"To perform classification\", \"opt4\": \"To handle positional information\"}, {\"question\": \"What is the main purpose of the decoder in a Transformer model?\", \"opt1\": \"To process and transform the input sequence\", \"opt2\": \"To generate the final output sequence\", \"opt3\": \"To perform classification\", \"opt4\": \"To handle positional information\"}, {\"question\": \"What is positional encoding in Transformer models used for?\", \"opt1\": \"To capture semantic meaning of words\", \"opt2\": \"To provide information about the relative position of tokens\", \"opt3\": \"To reduce computational complexity\", \"opt4\": \"To handle variable-length sequences\"}, {\"question\": \"What are residual connections in Transformer models used for?\", \"opt1\": \"To prevent vanishing gradients\", \"opt2\": \"To improve training stability\", \"opt3\": \"To enhance the model's learning capacity\", \"opt4\": \"All of the above\"}, {\"question\": \"What is Layer Normalization in Transformer models used for?\", \"opt1\": \"To normalize the inputs\", \"opt2\": \"To stabilize training\", \"opt3\": \"To normalize the activations within each layer\", \"opt4\": \"To reduce the number of parameters\"}, {\"question\": \"What is the role of the 'block' in the Transformer encoder?\", \"opt1\": \"To perform input embedding\", \"opt2\": \"To apply self-attention\", \"opt3\": \"To apply layer normalization\", \"opt4\": \"All of the above\"}, {\"question\": \"What does the term 'Multi-Head Attention' refer to?\", \"opt1\": \"Using multiple sets of weights to capture different aspects of the input\", \"opt2\": \"Combining information from different sources\", \"opt3\": \"Processing the input in parallel\", \"opt4\": \"All of the above\"}, {\"question\": \"How does a Transformer decoder differ from its encoder?\", \"opt1\": \"The decoder uses masked self-attention\", \"opt2\": \"The decoder generates the output sequence\", \"opt3\": \"The decoder utilizes cross-attention\", \"opt4\": \"All of the above\"}, {\"question\": \"What is 'masked' self-attention used for?\", \"opt1\": \"To prevent the decoder from attending to future tokens\", \"opt2\": \"To improve computational efficiency\", \"opt3\": \"To enhance the model's ability to capture long-range dependencies\", \"opt4\": \"To reduce the number of parameters\"}, {\"question\": \"What is the purpose of cross-attention in Transformer models?\", \"opt1\": \"To allow the decoder to attend to the encoder's output\", \"opt2\": \"To combine information from different input sequences\", \"opt3\": \"To improve the model's ability to capture long-range dependencies\", \"opt4\": \"All of the above\"}, {\"question\": \"What is 'teacher forcing' used for in training Transformer models?\", \"opt1\": \"To provide the decoder with the ground truth during training\", \"opt2\": \"To prevent overfitting\", \"opt3\": \"To speed up training\", \"opt4\": \"To improve generalization performance\"}, {\"question\": \"What type of task is particularly well-suited for a sequence-to-sequence model?\", \"opt1\": \"Image classification\", \"opt2\": \"Machine translation\", \"opt3\": \"Time series forecasting\", \"opt4\": \"Object detection\"}]\n",
    "answers=[{\"question\": \"What is the primary function of a Transformer model?\", \"answer\": \"Sequence-to-sequence modeling\", \"explanation\": \"Transformer models are designed for sequence-to-sequence tasks, processing input sequences and generating corresponding output sequences.\"}, {\"question\": \"What is a key characteristic of Transformer models regarding output length?\", \"answer\": \"Variable and determined by the model\", \"explanation\": \"The output length in Transformer models is not fixed and is dynamically determined by the model itself.\"}, {\"question\": \"What technique do Transformer models primarily use instead of recurrence for sequential processing?\", \"answer\": \"Self-attention\", \"explanation\": \"Transformer models use self-attention mechanisms instead of recurrent connections, enabling parallelization and handling of long-range dependencies.\"}, {\"question\": \"What is the main purpose of the encoder in a Transformer model?\", \"answer\": \"To process and transform the input sequence\", \"explanation\": \"The encoder processes the input sequence and transforms it into a representation suitable for further processing by the decoder.\"}, {\"question\": \"What is the main purpose of the decoder in a Transformer model?\", \"answer\": \"To generate the final output sequence\", \"explanation\": \"The decoder takes the encoded representation and generates the output sequence token by token.\"}, {\"question\": \"What is positional encoding in Transformer models used for?\", \"answer\": \"To provide information about the relative position of tokens\", \"explanation\": \"Positional encoding adds information about the position of each token in the input sequence, essential for understanding word order.\"}, {\"question\": \"What are residual connections in Transformer models used for?\", \"answer\": \"All of the above\", \"explanation\": \"Residual connections help to improve training stability, prevent vanishing gradients, and enhance learning by enabling the model to learn more complex patterns.\"}, {\"question\": \"What is Layer Normalization in Transformer models used for?\", \"answer\": \"To normalize the activations within each layer\", \"explanation\": \"Layer normalization stabilizes training by normalizing the activations within each layer, preventing internal covariate shift.\"}, {\"question\": \"What is the role of the 'block' in the Transformer encoder?\", \"answer\": \"All of the above\", \"explanation\": \"Each block in the Transformer encoder performs input embedding, self-attention, feed-forward, residual connections and layer normalization.\"}, {\"question\": \"What does the term 'Multi-Head Attention' refer to?\", \"answer\": \"All of the above\", \"explanation\": \"Multi-Head attention uses multiple sets of weights to attend to different aspects of the input, allowing for parallel processing and capturing diverse relationships.\"}, {\"question\": \"How does a Transformer decoder differ from its encoder?\", \"answer\": \"All of the above\", \"explanation\": \"The decoder uses masked self-attention to prevent attending to future tokens, generates the output sequence, and uses cross-attention to combine information from the encoder.\"}, {\"question\": \"What is 'masked' self-attention used for?\", \"answer\": \"To prevent the decoder from attending to future tokens\", \"explanation\": \"Masked self-attention prevents the decoder from 'peeking' ahead at future tokens when generating the output sequence.\"}, {\"question\": \"What is the purpose of cross-attention in Transformer models?\", \"answer\": \"All of the above\", \"explanation\": \"Cross-attention allows the decoder to focus on relevant parts of the encoder's output, improving the model's ability to capture long-range dependencies and combine information from both encoder and decoder.\"}, {\"question\": \"What is 'teacher forcing' used for in training Transformer models?\", \"answer\": \"To provide the decoder with the ground truth during training\", \"explanation\": \"Teacher forcing provides the decoder with the correct output during training, improving stability and speeding up training.\"}, {\"question\": \"What type of task is particularly well-suited for a sequence-to-sequence model?\", \"answer\": \"Machine translation\", \"explanation\": \"Sequence-to-sequence models are particularly well-suited for tasks that involve transforming one sequence into another, such as machine translation.\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mcq), len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the primary function of a Transformer model?',\n",
       " 'opt1': 'Image classification',\n",
       " 'opt2': 'Sequence-to-sequence modeling',\n",
       " 'opt3': 'Time series forecasting',\n",
       " 'opt4': 'Sentiment analysis'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the primary function of a Transformer model?',\n",
       " 'answer': 'Sequence-to-sequence modeling',\n",
       " 'explanation': 'Transformer models are designed for sequence-to-sequence tasks, processing input sequences and generating corresponding output sequences.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is a key characteristic of Transformer models regarding output length?',\n",
       " 'opt1': 'Fixed and predetermined',\n",
       " 'opt2': 'Variable and determined by the model',\n",
       " 'opt3': 'Dependent on input length',\n",
       " 'opt4': 'Independent of input sequence'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcq[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is a key characteristic of Transformer models regarding output length?',\n",
       " 'answer': 'Variable and determined by the model',\n",
       " 'explanation': 'The output length in Transformer models is not fixed and is dynamically determined by the model itself.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What technique do Transformer models primarily use instead of recurrence for sequential processing?',\n",
       " 'opt1': 'Convolution',\n",
       " 'opt2': 'Self-attention',\n",
       " 'opt3': 'Pooling',\n",
       " 'opt4': 'Recurrent connections'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcq[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What technique do Transformer models primarily use instead of recurrence for sequential processing?',\n",
       " 'answer': 'Self-attention',\n",
       " 'explanation': 'Transformer models use self-attention mechanisms instead of recurrent connections, enabling parallelization and handling of long-range dependencies.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the main purpose of the encoder in a Transformer model?',\n",
       " 'opt1': 'To generate the final output',\n",
       " 'opt2': 'To process and transform the input sequence',\n",
       " 'opt3': 'To perform classification',\n",
       " 'opt4': 'To handle positional information'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcq[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the main purpose of the encoder in a Transformer model?',\n",
       " 'answer': 'To process and transform the input sequence',\n",
       " 'explanation': 'The encoder processes the input sequence and transforms it into a representation suitable for further processing by the decoder.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the main purpose of the decoder in a Transformer model?',\n",
       " 'opt1': 'To process and transform the input sequence',\n",
       " 'opt2': 'To generate the final output sequence',\n",
       " 'opt3': 'To perform classification',\n",
       " 'opt4': 'To handle positional information'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcq[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the main purpose of the decoder in a Transformer model?',\n",
       " 'answer': 'To generate the final output sequence',\n",
       " 'explanation': 'The decoder takes the encoded representation and generates the output sequence token by token.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is positional encoding in Transformer models used for?',\n",
       " 'opt1': 'To capture semantic meaning of words',\n",
       " 'opt2': 'To provide information about the relative position of tokens',\n",
       " 'opt3': 'To reduce computational complexity',\n",
       " 'opt4': 'To handle variable-length sequences'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcq[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is positional encoding in Transformer models used for?',\n",
       " 'answer': 'To provide information about the relative position of tokens',\n",
       " 'explanation': 'Positional encoding adds information about the position of each token in the input sequence, essential for understanding word order.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are residual connections in Transformer models used for?',\n",
       " 'opt1': 'To prevent vanishing gradients',\n",
       " 'opt2': 'To improve training stability',\n",
       " 'opt3': \"To enhance the model's learning capacity\",\n",
       " 'opt4': 'All of the above'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcq[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are residual connections in Transformer models used for?',\n",
       " 'answer': 'All of the above',\n",
       " 'explanation': 'Residual connections help to improve training stability, prevent vanishing gradients, and enhance learning by enabling the model to learn more complex patterns.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is Layer Normalization in Transformer models used for?',\n",
       " 'opt1': 'To normalize the inputs',\n",
       " 'opt2': 'To stabilize training',\n",
       " 'opt3': 'To normalize the activations within each layer',\n",
       " 'opt4': 'To reduce the number of parameters'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcq[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is Layer Normalization in Transformer models used for?',\n",
       " 'answer': 'To normalize the activations within each layer',\n",
       " 'explanation': 'Layer normalization stabilizes training by normalizing the activations within each layer, preventing internal covariate shift.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"What is the role of the 'block' in the Transformer encoder?\",\n",
       " 'opt1': 'To perform input embedding',\n",
       " 'opt2': 'To apply self-attention',\n",
       " 'opt3': 'To apply layer normalization',\n",
       " 'opt4': 'All of the above'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcq[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"What is the role of the 'block' in the Transformer encoder?\",\n",
       " 'answer': 'All of the above',\n",
       " 'explanation': 'Each block in the Transformer encoder performs input embedding, self-attention, feed-forward, residual connections and layer normalization.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"What does the term 'Multi-Head Attention' refer to?\",\n",
       " 'opt1': 'Using multiple sets of weights to capture different aspects of the input',\n",
       " 'opt2': 'Combining information from different sources',\n",
       " 'opt3': 'Processing the input in parallel',\n",
       " 'opt4': 'All of the above'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcq[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"What does the term 'Multi-Head Attention' refer to?\",\n",
       " 'answer': 'All of the above',\n",
       " 'explanation': 'Multi-Head attention uses multiple sets of weights to attend to different aspects of the input, allowing for parallel processing and capturing diverse relationships.'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How does a Transformer decoder differ from its encoder?',\n",
       " 'opt1': 'The decoder uses masked self-attention',\n",
       " 'opt2': 'The decoder generates the output sequence',\n",
       " 'opt3': 'The decoder utilizes cross-attention',\n",
       " 'opt4': 'All of the above'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcq[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How does a Transformer decoder differ from its encoder?',\n",
       " 'answer': 'All of the above',\n",
       " 'explanation': 'The decoder uses masked self-attention to prevent attending to future tokens, generates the output sequence, and uses cross-attention to combine information from the encoder.'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"What is 'masked' self-attention used for?\",\n",
       " 'opt1': 'To prevent the decoder from attending to future tokens',\n",
       " 'opt2': 'To improve computational efficiency',\n",
       " 'opt3': \"To enhance the model's ability to capture long-range dependencies\",\n",
       " 'opt4': 'To reduce the number of parameters'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcq[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"What is 'masked' self-attention used for?\",\n",
       " 'answer': 'To prevent the decoder from attending to future tokens',\n",
       " 'explanation': \"Masked self-attention prevents the decoder from 'peeking' ahead at future tokens when generating the output sequence.\"}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the purpose of cross-attention in Transformer models?',\n",
       " 'opt1': \"To allow the decoder to attend to the encoder's output\",\n",
       " 'opt2': 'To combine information from different input sequences',\n",
       " 'opt3': \"To improve the model's ability to capture long-range dependencies\",\n",
       " 'opt4': 'All of the above'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcq[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the purpose of cross-attention in Transformer models?',\n",
       " 'answer': 'All of the above',\n",
       " 'explanation': \"Cross-attention allows the decoder to focus on relevant parts of the encoder's output, improving the model's ability to capture long-range dependencies and combine information from both encoder and decoder.\"}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"What is 'teacher forcing' used for in training Transformer models?\",\n",
       " 'opt1': 'To provide the decoder with the ground truth during training',\n",
       " 'opt2': 'To prevent overfitting',\n",
       " 'opt3': 'To speed up training',\n",
       " 'opt4': 'To improve generalization performance'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcq[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"What is 'teacher forcing' used for in training Transformer models?\",\n",
       " 'answer': 'To provide the decoder with the ground truth during training',\n",
       " 'explanation': 'Teacher forcing provides the decoder with the correct output during training, improving stability and speeding up training.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What type of task is particularly well-suited for a sequence-to-sequence model?',\n",
       " 'opt1': 'Image classification',\n",
       " 'opt2': 'Machine translation',\n",
       " 'opt3': 'Time series forecasting',\n",
       " 'opt4': 'Object detection'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcq[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What type of task is particularly well-suited for a sequence-to-sequence model?',\n",
       " 'answer': 'Machine translation',\n",
       " 'explanation': 'Sequence-to-sequence models are particularly well-suited for tasks that involve transforming one sequence into another, such as machine translation.'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[14]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
